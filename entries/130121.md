# 13th January 2020

## Agenda for today's meeting

1. Meeting with Will!
2. If there's time, discussion of the results presented below
3. We left the MHC locus out from the Q-Q plots I prepared earlier. Is there any insight to be gained from leaving the MHC locus out of cFDR analyses?
4. Next steps now that we have fairly definitive cFDR results 

## TODOs from last meeting 

1. Run Anna's method: 
  * use lupus and PID, 
  * How do the newly significant SNPs differ from those for cFDR? 
  * Construct Manhattan plots for SLE and PID overlap; are the novel SNPs near already-significant loci?
2. Writing on FDR, BH, etc.
3. `Rcpp` implementation of `cfdr::vl`

## What I've done this week

### `Rcpp` implementation of `cfdr::vl`

My `Rcpp` implementation of `cfdr::vl` (which I identified as the bottleneck in the `cfdr` procedure) seems to reproduce the behaviour of the R implementation and achieves a >3x speed-up. My implementation is limited to 'mode 2' and is thus not a drop-in replacement for `cfdr::vl` in all use cases. 

I reproduced the results of the `cfdr` vignette using this implementation, then confirmed the same using p-values from PID and the ASTER data set (one of the nine IMD datasets). I was able to run cFDR analyses inside of 8 hours with a p-value threshold of 0.01, something which had not been possible for all of the data sets before.

However, it was still not possible to complete a cFDR analysis for any of the nine data sets inside of 8 hours with a p-value threshold 0.1. As one expects from a uniformly distributed random variable, increasing this threshold by an order of magnitude increases the number of p-values considered by approximately the same factor.

| dataset     | pThreshold |   count |
|-------------|------------|---------|
| aster       |          1 | 6580715 |
| aster       |        0.1 |  683974 |
| aster       |       0.01 |   75955 |
| cad         |          1 | 6049564 |
| cad         |        0.1 |  630625 |
| cad         |       0.01 |   70252 |
| cd          |          1 | 6531004 |
| cd          |        0.1 |  675331 |
| cd          |       0.01 |   74269 |
| ra_okada_1  |          1 | 6109321 |
| ra_okada_1  |        0.1 |  637740 |
| ra_okada_1  |       0.01 |   71199 |
| sle         |          1 | 5759849 |
| sle         |        0.1 |  602528 |
| sle         |       0.01 |   67840 |
| t1d         |          1 | 5667872 |
| t1d         |        0.1 |  592907 |
| t1d         |       0.01 |   67180 |
| t2d         |          1 | 6920894 |
| t2d         |        0.1 |  716081 |
| t2d         |       0.01 |   78677 |
| uc          |          1 | 6539755 |
| uc          |        0.1 |  676242 |
| uc          |       0.01 |   74357 |
| ukbb_asthma |          1 | 5588407 |
| ukbb_asthma |        0.1 |  582349 |
| ukbb_asthma |       0.01 |   64815 |

### Revised cFDR results at p-value cut-off of 0.01

![No. of rejected nulls for v- and p-values with the new p-value cut-off of 0.01](/images/130121/pid_cfdr_1e-2.png)

### But what difference does it make, anyway?

Do we get many more hits at relevant FDRs by including larger p-values in the cFDR procedure (rather than just setting them to 1)?

![](/images/130121/pThresholdAndHits_1.png)
![](/images/130121/pThresholdAndHits_2.png)
![](/images/130121/pThresholdAndHits_3.png)

The additional number of hits at FDRs of 0.01 and 0.001 seems to tail off as we approach 1e-2, the current p-value threshold I am using. Is it worth going higher? I think probably not.

### `fcfdr`

No progress here. As my `Rcpp` implementation began to look promising, I could not help myself and kept working on it.

### Reading and writing

I'm drafting a section on multiple testing and FDR, but it is not ready yet.
